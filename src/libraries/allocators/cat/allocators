// -*- mode: c++ -*-
// vim: set ft=cpp:
#pragma once

#include <cat/bit>
#include <cat/linux>
#include <cat/meta>
#include <cat/optional>

namespace cat {

template <typename T>
struct BaseMemoryHandle {
    using Type = T;
    // Memory handles can represent either a pointer to stable data, or an
    // address offset to unstable data.
    union {
        T* p_storage;
        ssize storage;
    };
};

namespace detail {
    constexpr int small_size_threshold = 256;

    template <typename Handle>
    struct InlineMemoryHandle {
        using Type = typename Handle::Type;
        union {
            Handle handle;
            Type inline_storage;
            // Reserve `small_size_threshold` bytes to make the size
            // of this handle predictible.
            Byte empty[detail::small_size_threshold];
        };
        bool is_on_stack;

        InlineMemoryHandle(Handle const& handle)
            : handle(handle), is_on_stack(false){};

        InlineMemoryHandle(Type const& value)
            : inline_storage(value), is_on_stack(true){};

        ~InlineMemoryHandle() {
            if (this->is_on_stack) {
                this->inline_storage.~Type();
            }
        }
    };
}  // namespace detail

template <typename AllocatorT, typename AllocationU = void*>
concept Allocator = requires(AllocatorT allocator) {
    // Every allocator has a `.alloc()` method.
    allocator.alloc(ssizeof<AllocationU>());
    allocator.template alloc<AllocationU>();

    // TODO: It would be nice if these could make the concept more precise:
    // allocator.free(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
    // allocator.get(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
};

namespace detail {
    // `clangd` emits false-positive diagnostics if this constraint is used
    // anonymously without a `concept`.
    template <typename AllocatorT>
    concept StableDerivedAllocator = AllocatorT::has_pointer_stability == true;

    template <typename T>
    concept HasReset = requires(T allocator) {
        allocator.reset();
    };
}  // namespace detail

template <typename AllocatorT, typename AllocationU = void*>
concept StableAllocator = requires(AllocatorT it) {
    it.template p_alloc<AllocationU>();
};

template <typename Derived>
class AllocatorFacade {
    constexpr auto self() -> Derived& {
        return static_cast<Derived&>(*this);
    }

    // Take a pointer to another member function, then invoke it, and propagate
    // the address of memory that it allocated.
    template <typename T, typename... Args>
    auto p_wrap_allocation(auto function, Args&&... arguments)
        -> OptionalPtr<T> {
        Optional memory = (this->*function)(forward<Args>(arguments)...);
        if (memory.has_value()) {
            return &this->get(memory.value());
        }
        return nullptr;
    }

  public:
    // TODO: Forward `T` constructor arguments.
    template <typename T, bool is_small_optimized, bool is_fail_safe,
              bool is_aligned>
    auto meta_alloc(usize alignment, ssize allocation_size) {
        // Produce a basic handle for this memory type.
        using UnderlyingHandle = decltype(this->self().template make_handle<T>(
            T{}, allocation_size));

        // Get the return value of a user-supplied `.allocate()`. This must be
        // some kind of `Optional`. It holds some data that can pass into a
        // `Handle`'s constructor.
        using MaybeAllocation =
            decltype(this->self().template allocate<T>(allocation_size));

        // Produce an appropriate handle type for this allocation.
        using Handle = Conditional<is_small_optimized,
                                   // If this small-size-optimized:
                                   detail::InlineMemoryHandle<UnderlyingHandle>,
                                   // If this is not small-size optimized:
                                   UnderlyingHandle>;

        if constexpr (is_small_optimized) {
            if (allocation_size < detail::small_size_threshold) {
                // Allocate memory on this stack frame.
                T stack_memory;
                Handle stack_handle = stack_memory;
                // Return here to skip error handling, because an on-stack
                // allocation cannot fail.
                return Optional<Handle>(stack_handle);
            }
        }

        // Initialize an empty `Optional`.
        MaybeAllocation maybe_memory;

        // If this is not small size optimized, or is larger than
        // `small_size_threshold`:
        if constexpr (is_aligned) {
            maybe_memory = this->self().template aligned_allocate<T>(
                alignment, allocation_size);
        } else {
            maybe_memory = this->self().template allocate<T>(allocation_size);
        }

        if constexpr (is_fail_safe) {
            if (!maybe_memory.has_value()) {
                // Return an empty optional if this failed to allocate.
                return Optional<Handle>(nullopt);
            }
        }

        UnderlyingHandle handle = this->self().template make_handle<T>(
            maybe_memory.value(), allocation_size);

        Handle aligned_handle = Handle{handle};
        // `.access()` produces a pointer to the data, which is
        // constructed in-place.
        new (this->self().access(handle)) T();

        if constexpr (is_fail_safe) {
            return Optional<Handle>(aligned_handle);
        } else {
            return aligned_handle;
        }
    }

    // Try to allocate some memory with arbitrary alignment.
    template <typename T = Byte>
    [[nodiscard]] auto alloc(ssize allocation_size = ssizeof<T>()) {
        if constexpr (alignof(T) > 1) {
            // If the alignment of `T` is greater than 1, it should have
            // implied strong alignment guarantees.
            return this->align_alloc<T>(alignof(T), allocation_size);
        } else {
            return this->meta_alloc<T, false, true, false>(1u, allocation_size);
        }
    }

    // If this allocator has pointer stability, allocate some memory with
    // arbitrary alignment and provide a pointer handle.
    template <typename T = Byte>
        requires(detail::StableDerivedAllocator<Derived>)
    [[nodiscard]] auto p_alloc(ssize allocation_size = ssizeof<T>())
        -> OptionalPtr<T> {
        return this->p_wrap_allocation<T>(&AllocatorFacade<Derived>::alloc<T>,
                                          allocation_size);
    }

    // Try to allocate some memory guaranteed to be aligned to some
    // boundary.
    template <typename T = Byte>
    [[nodiscard]] auto align_alloc(usize alignment,
                                   ssize allocation_size = ssizeof<T>()) {
        return this->meta_alloc<T, false, true, true>(alignment,
                                                      allocation_size);
    }

    // If this allocator has pointer stability, allocate some memory with
    // arbitrary alignment and provide a pointer handle.
    template <typename T = Byte>
        requires(detail::StableDerivedAllocator<Derived>)
    [[nodiscard]] auto p_align_alloc(usize alignment,
                                     ssize allocation_size = ssizeof<T>())
        -> OptionalPtr<T> {
        return this->p_wrap_allocation<T>(
            &AllocatorFacade<Derived>::align_alloc<T>, alignment,
            allocation_size);
    }

    // Try to allocate small-size optimized memory.
    template <typename T = Byte>
    [[nodiscard]] auto inline_alloc(ssize allocation_size = ssizeof<T>()) {
        if constexpr (alignof(T) > 1) {
            // If the alignment of `T` is greater than 1, it should have
            // implied strong alignment guarantees.
            return this->inline_align_alloc<T>(alignof(T), allocation_size);
        } else {
            return this->meta_alloc<T, true, true, false>(1u, allocation_size);
        }
    }

    template <typename T = Byte>
        requires(detail::StableDerivedAllocator<Derived>)
    [[nodiscard]] auto p_inline_alloc(ssize allocation_size = ssizeof<T>()) {
        return this->p_wrap_allocation<T>(
            &AllocatorFacade<Derived>::align_alloc<T>, allocation_size);
    }

    // Try to allocate small-size optimized memory guaranteed to be aligned
    // to a boundary.
    template <typename T = Byte>
    [[nodiscard]] auto inline_align_alloc(
        usize alignment, ssize allocation_size = ssizeof<T>()) {
        return this->meta_alloc<T, true, true, true>(alignment,
                                                     allocation_size);
    }

    template <typename T = Byte>
        requires(detail::StableDerivedAllocator<Derived>)
    [[nodiscard]] auto p_inline_align_alloc(
        ssize allocation_size = ssizeof<T>()) {
        return this->p_wrap_allocation<T>(
            &AllocatorFacade<Derived>::inline_align_alloc<T>, allocation_size);
    }

    // TODO: Use a `Handle` `concept`.
    // Invalidate any memory handle, invoking its data's destructor.
    template <typename Handle>
    void free(Handle const& memory) {
        using Type = typename Handle::Type;

        // If this is a small-size optimized handle:
        if constexpr (IsSpecializationTrait<
                          Handle, detail::InlineMemoryHandle>::value) {
            if (memory.is_on_stack) {
                if constexpr (requires { this->get(memory).~Type(); }) {
                    this->get(memory).~Type();
                }
            } else {
                // If this was on stack, recurse with the underlying memory
                // handle to remove it from the allocator.
                this->free(memory.handle);
            }
        }
        // If this is not a small-size optimized handle:
        else {
            if constexpr (requires { this->get(memory).~Type(); }) {
                this->get(memory).~Type();
            }
            this->self().deallocate(memory);
        }
    };

    // Invalidate any stable memory handle, invoking its data's destructor.
    template <typename T>
        requires(detail::StableDerivedAllocator<Derived>)
    void free(T* p_memory) {
        if constexpr (requires { p_memory->~T(); }) {
            // p_memory->~T();
        }
        this->self().deallocate(p_memory);
    }

    // TODO: Use a `MemoryHandle` `concept`.
    // Get a non-`const` reference to the data in any memory handle.
    [[nodiscard]] auto get(auto& memory) -> auto& {
        using Handle = RemoveCvRef<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.inline_storage;
            }
            return *this->self().template access<Type>(memory.handle);
        } else {
            // Get non-small-size optimized data:
            return *this->self().template access<Type>(memory);
        }
    }

    // TODO: Can I use `cat::unconst()` here?
    // TODO: Use a `MemoryHandle` `concept`.
    // Get a `const` reference to the data in any memory handle.
    [[nodiscard]] auto get(auto const& memory) -> auto const& {
        using Handle = RemoveCvRef<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.inline_storage;
            }
            return *this->self().template access<Type>(memory.handle);
        } else {
            // Get non-small-size optimized data:
            return *this->self().template access<Type>(memory);
        }
    }

    // If the allocator does not provide a `.reset()` method, produce a
    // no-op.
    void reset() requires(!detail::HasReset<Derived>) {
    }
};

struct PageAllocator : public AllocatorFacade<PageAllocator> {
    static constexpr bool has_pointer_stability = true;

    friend AllocatorFacade<PageAllocator>;
  private:
    template <typename T>
    struct PageMemoryHandle : BaseMemoryHandle<T> {
        // This inherits:
        // `T* p_storage;`
        ssize allocation_size;
    };

    // Allocate memory in multiples of a page-size. A page is `4_ki` large
    // on x86-64. If fewer than 4096 bytes are allocated, that amount will
    // be rounded up to 4096.
    template <typename T>
    auto allocate(ssize allocation_size) -> OptionalPtr<T> {
        Scaredy result = nix::sys_mmap(
            0u, allocation_size,
            // TODO: Fix bit flags operators.
            static_cast<nix::MemoryProtectionFlags>(
                static_cast<unsigned int>(nix::MemoryProtectionFlags::read) |
                static_cast<unsigned int>(nix::MemoryProtectionFlags::write)),
            static_cast<nix::MemoryFlags>(
                static_cast<unsigned int>(nix::MemoryFlags::privately) |
                static_cast<unsigned int>(nix::MemoryFlags::populate) |
                static_cast<unsigned int>(nix::MemoryFlags::anonymous)),
            // Anonymous pages (non-files) must have `-1`.
            nix::FileDescriptor{-1},
            // Anonymous pages (non-files) must have `0`.
            0);
        if (result.has_value()) {
            return reinterpret_cast<T*>(static_cast<void*>(result.value()));
        }
        return nullptr;
    }

    // TODO: Use a `MemoryHandle` `concept`.
    // Unmap an opaque handle to page(s) of virtual memory.
    void deallocate(auto const& memory) {
        // There are some cases where `munmap` might fail even with private
        // anonymous pages. These currently cannot be handled, because `.free()`
        // does not propagate errors.
        _ = nix::sys_munmap(memory.p_storage, memory.allocation_size);
    };

    // Unmap a pointer handle to page(s) of virtual memory.
    template <typename T>
    void deallocate(T* p_memory) {
        // There are some cases where `munmap` might fail even with private
        // anonymous pages. These currently cannot be handled, because `.free()`
        // does not propagate errors.
        _ = nix::sys_munmap(p_memory, ssizeof<T>());
    };

    // Allocate a page(s) of virtual memory that is guaranteed to align to
    // any power of two, less than `4_ki`.
    template <typename T>
    auto aligned_allocate(usize alignment, ssize allocation_size)
        -> OptionalPtr<T> {
        Result(alignment <= 4_uki).assert();
        // A normal page allocation already has these semantics.
        return this->allocate<T>(allocation_size);
    }

    // TODO: Why `auto`?
    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize size) -> PageMemoryHandle<T> {
        return PageMemoryHandle<T>{{data}, size};
    }

    // Access a page(s) of virtual memory.
    template <typename T>
    auto access(PageMemoryHandle<T> const& memory) -> T* {
        return memory.p_storage;
    }
};

struct LinearAllocator : AllocatorFacade<LinearAllocator> {
    static constexpr bool has_pointer_stability = true;

    // TODO: Should this be `usize`?
    LinearAllocator(uintptr<void> p_address, ssize arena_size)
        : p_arena_begin(p_address.raw + arena_size.raw),
          p_arena_end(p_address.raw){};

    // Reset the bumped pointer to the beginning of this arena.
    void reset() {
        this->p_arena_current = p_arena_begin;
    }

    friend AllocatorFacade<LinearAllocator>;
  private:
    template <typename T>
    struct LinearMemoryHandle : BaseMemoryHandle<T> {
        // This inherits:
        // `T* p_storage;`
    };

    // These cannot hold `void` because that is not a regular type.
    // TODO: Support `uintptr<void>`.
    uintptr<Byte> const p_arena_begin;
    uintptr<Byte> const p_arena_end;
    uintptr<Byte> p_arena_current = p_arena_begin;

    // `LinearAllocator` makes allocations in precisely the size requested.
    // TODO: Due to alignment discrepencies when bumping down, this actually
    // could be larger.
    template <typename T>
    auto allocation_size(ssize allocation_size) -> ssize {
        return allocation_size;
    }

    // Try to allocate memory and bump the pointer down.
    template <typename T>
    auto allocate(ssize allocation_size) -> OptionalPtr<T> {
        this->p_arena_current -= allocation_size.raw;
        if (this->p_arena_current >= p_arena_end) {
            // Return a pointer that is then used to in-place construct a `T`.
            return reinterpret_cast<T*>(
                static_cast<Byte*>(this->p_arena_current));
        }
        return nullptr;
    }

    // In general, memory cannot be deallocated in a linear allocator, so
    // this function is no-op.
    // The parameter is `auto` because it might be
    // either an opaque handle or a pointer.
    void deallocate(auto const&){};

    // Try to allocate memory aligned to some boundary and bump the pointer
    // down.
    template <typename T>
    auto aligned_allocate(usize alignment, ssize allocation_size)
        -> OptionalPtr<T> {
        uintptr<Byte> p_allocation =
            align_down(this->p_arena_current - allocation_size.raw, alignment);
        if (p_allocation >= p_arena_end) {
            this->p_arena_current = p_allocation;
            // Return a pointer that is then used to in-place construct a `T`.
            return reinterpret_cast<T*>(
                static_cast<Byte*>(this->p_arena_current));
        }
        return nullptr;
    }

    // TODO: Why `auto`?
    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize /* unused */) -> LinearMemoryHandle<T> {
        return LinearMemoryHandle<T>{data};
    }

    // Access some memory.
    template <typename T>
    auto access(LinearMemoryHandle<T> const& memory) -> T* {
        return memory.p_storage;
    }
};

}  // namespace cat
