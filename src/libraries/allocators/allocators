// -*- mode: c++ -*-
// vim: set ft=cpp:
#pragma once

#include <linux>
#include <memory>
#include <optional>
#include <type_traits>

// TODO: Some allocators should be iterable.
// TODO: Add a random_access concept and a random_access_allocator concept.

namespace cat {

template <typename T>
struct BaseMemoryHandle {
    using Type = T;
    union {
        void* p_storage;
        ssize storage;
    };
};

namespace detail {
    constexpr ssize small_size_threshold = 256;

    template <typename Handle>
    struct SmallMemoryHandle {
        using Type = typename Handle::Type;
        union {
            Handle allocator_storage;
            Type stack_storage;
            // Reserve `small_size_threshold` bytes to make the size
            // of this handle predictible.
            cat::Byte empty[cat::detail::small_size_threshold];
        };
        bool1 is_on_stack;

        SmallMemoryHandle(Handle const& handle)
            : allocator_storage(handle), is_on_stack(false){};

        SmallMemoryHandle(typename Handle::Type const& value)
            : stack_storage(value), is_on_stack(true){};

        ~SmallMemoryHandle() {
            if (this->is_on_stack) {
                this->stack_storage.~Type();
            }
        }
    };

    template <typename Handle>
    struct AlignedMemoryHandle {
        using Type = typename Handle::Type;
        Handle storage;
        ssize alignment;

        AlignedMemoryHandle(Handle const& handle, ssize in_alignment)
            : storage(handle), alignment(in_alignment){};

        ~AlignedMemoryHandle() {
            this->storage.~Handle();
        }
    };
}  // namespace detail

}  // namespace cat

namespace meta {

template <typename AllocatorT, typename AllocationU = void*>
concept Allocator = requires(AllocatorT allocator) {
    // Every allocator has a `.malloc()` method.
    allocator.malloc(sizeof(AllocationU));
    allocator.template malloc<AllocationU>();

    // TODO: It would be nice if these could make the concept more precise:
    // allocator.free(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
    // allocator.get(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
};

}  // namespace meta

namespace cat {

template <typename Derived>
struct AllocatorFacade {
  protected:
    constexpr auto self() -> Derived& {
        return static_cast<Derived&>(*this);
    }

    // TODO: Forward `T` constructor arguments.
    template <typename T, bool is_small_optimized, bool is_fail_safe,
              bool is_aligned>
    auto meta_allocate(ssize const alignment,
                       ssize const allocation_size = sizeof(T)) {
        // Produce a basic handle for this memory type.
        using UnderlyingHandle = decltype(this->self().template make_handle<T>(
            T{}, allocation_size));

        // Get the return value of a user-supplied `.allocate()`. This must be
        // a container that holds `T*`.
        // TODO: Support `ssize`.
        using Allocation =
            decltype(this->self().template allocate<T>(allocation_size));

        // Produce an appropriate handle type for this allocation.
        using Handle = meta::Conditional<
            is_aligned,
            meta::Conditional<
                is_small_optimized,
                // If this is aligned and it is small-optimized:
                cat::detail::AlignedMemoryHandle<
                    cat::detail::SmallMemoryHandle<UnderlyingHandle>>,
                // If this is aligned but it is not
                // small-optimized:
                cat::detail::AlignedMemoryHandle<UnderlyingHandle>>,
            meta::Conditional<is_small_optimized,
                              // If this is not aligned but it is
                              // small-optimized:
                              cat::detail::SmallMemoryHandle<UnderlyingHandle>,
                              // If this is not aligned and it is not
                              // small-optimized:
                              UnderlyingHandle>>;

        // Initialize either an `Optional` or a `Result` with an empty value.
        Allocation maybe_memory = meta::Conditional<
            meta::IsSpecializationTrait<Allocation, Optional>::value,
            cat::detail::NullOpt, Failure>{};

        if constexpr (!is_aligned) {
            if constexpr (is_small_optimized) {
                if (allocation_size < cat::detail::small_size_threshold) {
                    // Allocate memory on this stack frame and return it, iff
                    // that would be smaller than
                    // cat::detail::small_size_threshold bytes.
                    T stack_memory;
                    return Optional<Handle>(T{});
                }
            }
            maybe_memory = this->self().template allocate<T>(allocation_size);
        } else {
            maybe_memory = this->self().template aligned_allocate<T>(
                alignment, allocation_size);
        }

        if constexpr (is_fail_safe) {
            // `allocate()` returns either an `Optional` or a `Result`, both of
            // which have `.has_value()`.
            if (!maybe_memory.has_value()) {
                // Return an empty optional if this failed to allocate.
                return Optional<Handle>(nullopt);
            }
        }

        UnderlyingHandle handle = this->self().template make_handle<T>(
            maybe_memory.value(), allocation_size);

        // TODO: It should be possible to streamline this:
        if constexpr (is_aligned) {
            Handle aligned_handle = Handle{handle, alignment};
            // `.access()` produces a pointer to the data, which is constructed
            // in-place.
            new (this->self().access(handle)) T();

            if constexpr (is_fail_safe) {
                return Optional<Handle>(aligned_handle);
            } else {
                return aligned_handle;
            }
        } else {
            // `.access()` produces a pointer to the data, which is constructed
            // in-place.
            new (this->self().access(handle)) T();

            if constexpr (is_fail_safe) {
                return Optional<Handle>(Handle{handle});
            } else {
                return Handle{handle};
            }
        }
    }
  public:
    // Try to allocate some memory with arbitrary alignment.
    template <typename T = cat::Byte>
    auto malloc(ssize const allocation_size = sizeof(T)) {
        return this->meta_allocate<T, false, true, false>(1, allocation_size);
    }

    // Try to allocate some memory guaranteed to be aligned to some
    // boundary.
    template <typename T = cat::Byte>
    auto aligned_alloc(ssize const alignment,
                       ssize const allocation_size = sizeof(T)) {
        return this->meta_allocate<T, false, true, true>(alignment,
                                                         allocation_size);
    }

    // Try to allocate small-size optimized memory.
    template <typename T = void*>
    auto malloca(ssize const allocation_size = sizeof(T)) {
        return this->meta_allocate<T, true, true, false>(1, allocation_size);
    }

    // Try to allocate small-size optimized memory guaranteed to be aligned
    // to a boundary. TODO: Work on this after improving `malloca()`.
    template <typename T = void*>
    auto aligned_malloca(ssize const alignment,
                         ssize const allocation_size = sizeof(T)) {
        return this->meta_allocate<T, true, true, true>(alignment,
                                                        allocation_size);
    }

    // Invalidate any memory handle, invoking its data's destructor.
    template <typename Handle>
    auto free(Handle const& memory) -> Result<> {
        using Type = typename Handle::Type;

        // If this is a small-size optimized handle:
        if constexpr (meta::IsSpecializationTrait<
                          Handle, cat::detail::SmallMemoryHandle>::value) {
            if (memory.is_on_stack) {
                if constexpr (requires { this->get(memory).~Type(); }) {
                    this->get(memory).~Type();
                }
                return okay;
            }
            // Recurse with the underlying memory handle.
            return this->free(memory.allocator_storage);
        }
        // If this is not a small-size optimized handle:
        else {
            if constexpr (requires { this->get(memory).~Type(); }) {
                this->get(memory).~Type();
            }

            if constexpr (requires { memory.alignment; }) {
                auto result = this->self().aligned_deallocate(memory.storage);
                if (result.has_value()) {
                    return okay;
                }
                return Failure(1);
            } else {
                auto result = this->self().deallocate(memory);
                if (result.has_value()) {
                    return okay;
                }
                return Failure(1);
            }
        }
        __builtin_unreachable();
    };

    // Get a reference to the data in any memory handle.
    auto get(auto& memory) -> auto& {
        using Handle = typename meta::RemoveCvref<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.stack_storage;
            }
            return *this->self().template access<Type>(
                memory.allocator_storage);
        } else {
            // Get non-small-size optimized data:
            if constexpr (requires { memory.alignment; }) {
                // Get aligned data:
                return *this->self().template access<Type>(memory.storage);
            } else {
                // Get not aligned data:
                return *this->self().template access<Type>(memory);
            }
        }
    }

    // Get a `const` reference to the data in any memory handle.
    auto get(auto const& memory) -> auto const& {
        using Handle = typename meta::RemoveCvref<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.stack_storage;
            }
            return *this->self().template access<Type>(
                memory.allocator_storage);
        } else {
            // Get non-small-size optimized data:
            if constexpr (requires { memory.alignment; }) {
                // Get aligned data:
                return *this->self().template access<Type>(memory.storage);
            } else {
                // Get not aligned data:
                return *this->self().template access<Type>(memory);
            }
        }
    }

    // If the allocator does not provide a `.reset()` method, produce a no-op.
    void reset() requires(!Derived::reset()) {
    }
};

struct PageAllocator : AllocatorFacade<PageAllocator> {
    template <typename T>
    struct PageMemoryHandle : BaseMemoryHandle<T> {
        ssize allocation_size;
    };

    // Allocate memory in multiples of a page-size. A page is `4_ki` large
    // on x86-64. If fewer that `4096` bytes are allocated, that amount will
    // be rounded up to `4096`.
    template <typename T>
    auto allocate(ssize const allocation_size) -> Result<void*> {
        return nix::map_memory(0u, allocation_size,
                               nix::MemoryProtectionFlags::read |
                                   nix::MemoryProtectionFlags::write,
                               nix::MemoryFlags::privately |
                                   nix::MemoryFlags::populate |
                                   nix::MemoryFlags::anonymous,
                               // Anonymous pages must have `-1`.
                               -1,
                               // Anonymous pages must have `0u`.
                               0u);
    }

    // Unmap a handle to page(s) of virtual memory.
    auto deallocate(auto const& memory) -> Result<> {
        return nix::unmap_memory(memory.p_storage, memory.allocation_size);
    };

    // Allocate a page(s) of virtual memory that is guaranteed to align to
    // any power of two, less than `4_ki`.
    template <typename T>
    auto aligned_allocate(ssize const alignment, ssize const allocation_size)
        -> Result<void*> {
        Result(alignment <= 4_ki).assert();
        // A normal page allocation already has these semantics.
        return this->allocate<T>(allocation_size);
    }

    // Unmap a handle to page(s) of aligned virtual memory.
    auto aligned_deallocate(auto const& memory) -> Result<> {
        // There are no special semantics for deallocation aligned memory.
        return nix::unmap_memory(memory.p_storage, memory.allocation_size);
    };

    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize size) -> PageMemoryHandle<T> {
        return PageMemoryHandle<T>{{data}, size};
    }

    // Access a page(s) of virtual memory.
    template <typename T>
    auto access(PageMemoryHandle<T> const& memory) -> T* {
        return static_cast<T*>(memory.p_storage);
    }
};

struct LinearAllocator : AllocatorFacade<LinearAllocator> {
    template <typename T>
    struct LinearMemoryHandle : BaseMemoryHandle<T> {
        ssize allocation_size;
    };
    intptr const p_arena_begin;
    intptr const p_arena_end;
    intptr p_arena_current = p_arena_begin;

    LinearAllocator(void* const p_address, ssize const arena_size)
        : p_arena_begin(intptr{p_address} + arena_size),
          p_arena_end(p_address){};

    // Try to allocate memory and bump the pointer.
    template <typename T>
    auto allocate(ssize const allocation_size) -> Result<void*> {
        intptr p_allocation = p_arena_current - allocation_size;
        if (p_allocation >= p_arena_end) {
            p_arena_current = p_allocation;
            return static_cast<void*>(p_arena_current);
        }
        return Failure(1);
    }

    // In general, memory cannot be deallocated in a linear allocator, so this
    // function is no-op.
    auto deallocate(auto const&) -> Result<> {
        return okay;
    };

    // Try to allocate memory aligned to some boundary and bump the pointer.
    template <typename T>
    auto aligned_allocate(ssize const alignment, ssize const allocation_size)
        -> Result<void*> {
        intptr p_allocation =
            cat::align_down(p_arena_current - allocation_size, alignment);
        if (p_allocation >= p_arena_end) {
            p_arena_current = p_allocation;
            return static_cast<void*>(p_arena_current);
        }
        return Failure(1);
    }

    // In general, memory cannot be deallocated in a linear allocator, so this
    // function is no-op.
    auto aligned_deallocate(auto const&) -> Result<> {
        return okay;
    };

    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize size) -> LinearMemoryHandle<T> {
        return LinearMemoryHandle<T>{{data}, size};
    }

    // Access some memory.
    template <typename T>
    auto access(LinearMemoryHandle<T> const& memory) -> T* {
        return static_cast<T*>(memory.p_storage);
    }

    // Reset the bumped pointer to the beginning of this arena.
    void reset() {
        p_arena_current = p_arena_begin;
    }
};

}  // namespace cat
