// -*- mode: c++ -*-
// vim: set ft=cpp:
#pragma once

#include <bit>
#include <linux>
#include <meta>
#include <optional>

namespace cat {

template <typename T>
struct BaseMemoryHandle {
    using Type = T;
    union {
        void* p_storage;
        ssize storage;
    };
};

namespace detail {
    constexpr int small_size_threshold = 256;

    template <typename Handle>
    struct SmallMemoryHandle {
        using Type = typename Handle::Type;
        union {
            Handle allocator_storage;
            Type stack_storage;
            // Reserve `small_size_threshold` bytes to make the size
            // of this handle predictible.
            cat::Byte empty[cat::detail::small_size_threshold];
        };
        bool1 is_on_stack;

        SmallMemoryHandle(Handle const& handle)
            : allocator_storage(handle), is_on_stack(false){};

        SmallMemoryHandle(typename Handle::Type const& value)
            : stack_storage(value), is_on_stack(true){};

        ~SmallMemoryHandle() {
            if (this->is_on_stack) {
                this->stack_storage.~Type();
            }
        }
    };

    template <typename Handle>
    struct AlignedMemoryHandle {
        using Type = typename Handle::Type;
        Handle storage;
        usize alignment;

        AlignedMemoryHandle(Handle const& handle, usize in_alignment)
            : storage(handle), alignment(in_alignment){};

        ~AlignedMemoryHandle() {
            this->storage.~Handle();
        }
    };
}  // namespace detail

}  // namespace cat

namespace meta {

template <typename AllocatorT, typename AllocationU = void*>
concept Allocator = requires(AllocatorT allocator) {
    // Every allocator has a `.malloc()` method.
    allocator.malloc(ssizeof<AllocationU>());
    allocator.template malloc<AllocationU>();

    // TODO: It would be nice if these could make the concept more precise:
    // allocator.free(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
    // allocator.get(decltype(allocator.template make_handle<AllocationU>(
    //     AllocationU{}, sizeof(AllocationU))){});
};

}  // namespace meta

namespace cat {

template <typename Derived>
struct AllocatorFacade {
  protected:
    constexpr auto self() -> Derived& {
        return static_cast<Derived&>(*this);
    }

    // TODO: Forward `T` constructor arguments.
    template <typename T, bool1 is_small_optimized, bool1 is_fail_safe,
              bool1 is_aligned>
    auto meta_allocate(usize const alignment,
                       ssize const allocation_size = ssizeof<T>()) {
        // Produce a basic handle for this memory type.
        using UnderlyingHandle = decltype(this->self().template make_handle<T>(
            T{}, allocation_size));

        // Get the return value of a user-supplied `.allocate()`. This must be
        // a container that holds `T*`.
        // TODO: Support `ssize`.
        using Allocation =
            decltype(this->self().template allocate<T>(allocation_size));

        // Produce an appropriate handle type for this allocation.
        using Handle = ::meta::Conditional<
            is_aligned,
            ::meta::Conditional<
                is_small_optimized,
                // If this is aligned and it is small-optimized:
                cat::detail::AlignedMemoryHandle<
                    cat::detail::SmallMemoryHandle<UnderlyingHandle>>,
                // If this is aligned but it is not
                // small-optimized:
                cat::detail::AlignedMemoryHandle<UnderlyingHandle>>,
            ::meta::Conditional<
                is_small_optimized,
                // If this is not aligned but it is
                // small-optimized:
                cat::detail::SmallMemoryHandle<UnderlyingHandle>,
                // If this is not aligned and it is not
                // small-optimized:
                UnderlyingHandle>>;

        // Initialize an empty `Optional`.
        Allocation maybe_memory;

        if constexpr (!is_aligned) {
            if constexpr (is_small_optimized) {
                if (allocation_size < cat::detail::small_size_threshold) {
                    // Allocate memory on this stack frame and return it, iff
                    // that would be smaller than
                    // `cat::detail::small_size_threshold`.
                    T stack_memory;
                    Handle stack_handle = stack_memory;
                    return Optional<Handle>(stack_handle);
                }
            }
            maybe_memory = this->self().template allocate<T>(allocation_size);
        } else {
            maybe_memory = this->self().template aligned_allocate<T>(
                alignment, allocation_size);
        }

        if constexpr (is_fail_safe) {
            // `allocate()` returns either an `Optional` or a `Result`, both of
            // which have `.has_value()`.
            if (!maybe_memory.has_value()) {
                // Return an empty optional if this failed to allocate.
                return Optional<Handle>(nullopt);
            }
        }

        UnderlyingHandle handle = this->self().template make_handle<T>(
            maybe_memory.value(), allocation_size);

        // TODO: It should be possible to streamline this:
        if constexpr (is_aligned) {
            Handle aligned_handle = Handle{handle, alignment};
            // `.access()` produces a pointer to the data, which is constructed
            // in-place.
            new (this->self().access(handle)) T();

            if constexpr (is_fail_safe) {
                return Optional<Handle>(aligned_handle);
            } else {
                return aligned_handle;
            }
        } else {
            // `.access()` produces a pointer to the data, which is constructed
            // in-place.
            new (this->self().access(handle)) T();

            if constexpr (is_fail_safe) {
                return Optional<Handle>(Handle{handle});
            } else {
                return Handle{handle};
            }
        }
    }
  public:
    // Try to allocate some memory with arbitrary alignment.
    template <typename T = cat::Byte>
    [[nodiscard]] auto malloc(ssize const allocation_size = ssizeof<T>()) {
        if constexpr (alignof(T) > 8) {
            // If the alignment of `T` is greater than 8, it should have implied
            // strong alignment guarantees.
            return this->aligned_alloc(alignof(T), allocation_size);
        }
        return this->meta_allocate<T, false, true, false>(1u, allocation_size);
    }

    // Try to allocate some memory guaranteed to be aligned to some
    // boundary.
    template <typename T = cat::Byte>
    [[nodiscard]] auto aligned_alloc(
        usize const alignment, ssize const allocation_size = ssizeof<T>()) {
        return this->meta_allocate<T, false, true, true>(alignment,
                                                         allocation_size);
    }

    // Try to allocate small-size optimized memory.
    template <typename T = void*>
    [[nodiscard]] auto malloca(ssize const allocation_size = ssizeof<T>()) {
        if constexpr (alignof(T) > 8) {
            // If the alignment of `T` is greater than 8, it should have implied
            // strong alignment guarantees.
            return this->aligned_alloca(alignof(T), allocation_size);
        }
        return this->meta_allocate<T, true, true, false>(1u, allocation_size);
    }

    // Try to allocate small-size optimized memory guaranteed to be aligned
    // to a boundary. TODO: Work on this after improving `malloca()`.
    template <typename T = void*>
    [[nodiscard]] auto aligned_malloca(
        usize const alignment, ssize const allocation_size = ssizeof<T>()) {
        return this->meta_allocate<T, true, true, true>(alignment,
                                                        allocation_size);
    }

    // Invalidate any memory handle, invoking its data's destructor.
    template <typename Handle>
    [[nodiscard]] auto free(Handle const& memory) -> cat::Optional<void> {
        using Type = typename Handle::Type;

        // If this is a small-size optimized handle:
        if constexpr (::meta::IsSpecializationTrait<
                          Handle, cat::detail::SmallMemoryHandle>::value) {
            if (memory.is_on_stack) {
                if constexpr (requires { this->get(memory).~Type(); }) {
                    this->get(memory).~Type();
                }
                return monostate;
            }
            // Recurse with the underlying memory handle.
            return this->free(memory.allocator_storage);
        }
        // If this is not a small-size optimized handle:
        else {
            if constexpr (requires { this->get(memory).~Type(); }) {
                this->get(memory).~Type();
            }

            if constexpr (requires { memory.alignment; }) {
                auto result = this->self().aligned_deallocate(memory.storage);
                if (result.has_value()) {
                    return monostate;
                }
                return nullopt;
            } else {
                auto result = this->self().deallocate(memory);
                if (result.has_value()) {
                    return monostate;
                }
                return nullopt;
            }
        }
        __builtin_unreachable();
    };

    // Get a reference to the data in any memory handle.
    [[nodiscard]] auto get(auto& memory) -> auto& {
        using Handle = typename ::meta::RemoveCvref<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.stack_storage;
            }
            return *this->self().template access<Type>(
                memory.allocator_storage);
        } else {
            // Get non-small-size optimized data:
            if constexpr (requires { memory.alignment; }) {
                // Get aligned data:
                return *this->self().template access<Type>(memory.storage);
            } else {
                // Get not aligned data:
                return *this->self().template access<Type>(memory);
            }
        }
    }

    // Get a `const` reference to the data in any memory handle.
    [[nodiscard]] auto get(auto const& memory) -> auto const& {
        using Handle = typename ::meta::RemoveCvref<decltype(memory)>;
        using Type = typename Handle::Type;
        if constexpr (requires { memory.is_on_stack; }) {
            // Get small-size optimized data:
            if (memory.is_on_stack) {
                return memory.stack_storage;
            }
            return *this->self().template access<Type>(
                memory.allocator_storage);
        } else {
            // Get non-small-size optimized data:
            if constexpr (requires { memory.alignment; }) {
                // Get aligned data:
                return *this->self().template access<Type>(memory.storage);
            } else {
                // Get not aligned data:
                return *this->self().template access<Type>(memory);
            }
        }
    }

    // If the allocator does not provide a `.reset()` method, produce a no-op.
    void reset() requires(!Derived::reset()) {
    }
};

struct PageAllocator : AllocatorFacade<PageAllocator> {
    template <typename T>
    struct PageMemoryHandle : BaseMemoryHandle<T> {
        ssize allocation_size;
    };

    // Allocate memory in multiples of a page-size. A page is `4_ki` large
    // on x86-64. If fewer that `4096` bytes are allocated, that amount will
    // be rounded up to `4096`.
    template <typename T>
    auto allocate(ssize const allocation_size) -> Optional<void*> {
        Scaredy result = nix::map_memory(
            0u, allocation_size,
            // TODO: Fix bit flags operators.
            static_cast<nix::MemoryProtectionFlags>(
                static_cast<unsigned int>(nix::MemoryProtectionFlags::read) |
                static_cast<unsigned int>(nix::MemoryProtectionFlags::write)),
            static_cast<nix::MemoryFlags>(
                static_cast<unsigned int>(nix::MemoryFlags::privately) |
                static_cast<unsigned int>(nix::MemoryFlags::populate) |
                static_cast<unsigned int>(nix::MemoryFlags::anonymous)),
            // Anonymous pages must have `-1`.
            nix::FileDescriptor{-1},
            // Anonymous pages must have `0`.
            0);
        if (result.has_value()) {
            return result.value();
        }
        return nullopt;
    }

    // Unmap a handle to page(s) of virtual memory.
    auto deallocate(auto const& memory) -> Optional<void> {
        Scaredy result =
            nix::unmap_memory(memory.p_storage, memory.allocation_size);
        if (result.has_value()) {
            return monostate;
        }
        return nullopt;
    };

    // Allocate a page(s) of virtual memory that is guaranteed to align to
    // any power of two, less than `4_ki`.
    template <typename T>
    auto aligned_allocate(usize const alignment, ssize const allocation_size)
        -> Optional<void*> {
        Result(alignment <= 4_uki).assert();
        // A normal page allocation already has these semantics.
        return this->allocate<T>(allocation_size);
    }

    // Unmap a handle to page(s) of aligned virtual memory.
    auto aligned_deallocate(auto const& memory) -> Optional<void> {
        // There are no special semantics for deallocation aligned memory.
        Scaredy result =
            nix::unmap_memory(memory.p_storage, memory.allocation_size);
        if (result.has_value()) {
            return monostate;
        }
        return nullopt;
    };

    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize size) -> PageMemoryHandle<T> {
        return PageMemoryHandle<T>{{data}, size};
    }

    // Access a page(s) of virtual memory.
    template <typename T>
    auto access(PageMemoryHandle<T> const& memory) -> T* {
        return static_cast<T*>(memory.p_storage);
    }
};

struct LinearAllocator : AllocatorFacade<LinearAllocator> {
    template <typename T>
    struct LinearMemoryHandle : BaseMemoryHandle<T> {
        ssize allocation_size;
    };

    // TODO: Use `AnyPtr`.
    uintptr<void> const p_arena_begin;
    uintptr<void> const p_arena_end;
    uintptr<void> p_arena_current = p_arena_begin;

    LinearAllocator(uintptr<void> const p_address, ssize const arena_size)
        : p_arena_begin(p_address + arena_size.c()), p_arena_end(p_address){};

    // Try to allocate memory and bump the pointer down.
    template <typename T>
    auto allocate(ssize const allocation_size) -> Optional<void*> {
        this->p_arena_current -= allocation_size.c();
        if (this->p_arena_current >= p_arena_end) {
            return static_cast<void*>(this->p_arena_current);
        }
        return nullopt;
    }

    // In general, memory cannot be deallocated in a linear allocator, so this
    // function is no-op.
    auto deallocate(auto const&) -> Optional<void> {
        return monostate;
    };

    // Try to allocate memory aligned to some boundary and bump the pointer
    // down.
    template <typename T>
    auto aligned_allocate(usize const alignment, ssize const allocation_size)
        -> Optional<void*> {
        uintptr<void> p_allocation = cat::align_down(
            this->p_arena_current - allocation_size.c(), alignment);
        if (p_allocation >= p_arena_end) {
            this->p_arena_current = p_allocation;
            return static_cast<void*>(this->p_arena_current);
        }
        return nullopt;
    }

    // In general, memory cannot be deallocated in a linear allocator, so this
    // function is no-op.
    auto aligned_deallocate(auto const&) -> Optional<void> {
        return monostate;
    };

    // Produce a handle to allocated memory.
    template <typename T>
    auto make_handle(auto data, ssize size) -> LinearMemoryHandle<T> {
        return LinearMemoryHandle<T>{{data}, size};
    }

    // Access some memory.
    template <typename T>
    auto access(LinearMemoryHandle<T> const& memory) -> T* {
        return static_cast<T*>(memory.p_storage);
    }

    // Reset the bumped pointer to the beginning of this arena.
    void reset() {
        this->p_arena_current = p_arena_begin;
    }
};

}  // namespace cat
